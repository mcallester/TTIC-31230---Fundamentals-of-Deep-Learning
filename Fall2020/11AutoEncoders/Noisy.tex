\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Autumn 2020}
  \vfill
  \centerline{Noisy Channel RDAs}
  \vfill
  \vfill


\slide{Rate-Distortion Autoencoders (RDAs)}

We compress a continuous signal $y$ to a bit string $\tilde{z}_\Phi(y)$.

\vfill
We decompress $\tilde{z}_\Phi(y)$ to $y_\Phi(\tilde{z}_\Phi(y))$.

\vfill
{\color{red} $$\Phi^* = \argmin_\Phi\; E_{y \sim \mathrm{Pop}}\;\;\;\;|\tilde{z}_\Phi(y)| + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y)))$$}

\slide{Rate-Distortion Autoencoders (RDAs)}

Since rounding is not differentiable we train by replace rounding by additive noise.

\vfill

\vfill
{\color{red} $${\cal L}(\Phi) = E_{y \sim \mathrm{Pop}}\;E_\epsilon \;\left\{\begin{array}{ll} &- \ln p_\Phi(z_\Phi(y) + \epsilon) \\
+ & \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon)) \end{array}\right.$$}

\vfill
A noisy-channel RDA uses the noise version without rounding.

\anaslide{Noisy Channel RDAs}

\bigskip
\bigskip
\begin{eqnarray*}
z & = & z_\Phi(y,\epsilon) \;\;\mbox{$\epsilon$ is fixed (parameter independent) noise} \\
\\
\Phi^* & = & \argmin_\Phi \;I_\Phi(y,z) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))
\end{eqnarray*}

\bigskip
By the channel capacity theorem {\color{red} $I(y,z)$} is the {\bf rate} of information transfer from $y$ to $z$.

\anaslide{Noisy Channel RDAs}

\bigskip
\bigskip
\begin{eqnarray*}
z & = & z_\Phi(y,\epsilon) \;\;\mbox{$\epsilon$ is fixed (parameter independent) noise} \\
\\
\Phi^* & = & \argmin_\Phi \;I_\Phi(y,z) + \lambda E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))
\end{eqnarray*}
        
\bigskip
Using parameter-independent noise is called the ``reparameterization trick'' and allows SGD.
\begin{eqnarray*}
& & \nabla_\Phi \;E_{y,\epsilon}\; \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon))) \\
\\
& = & E_{y,\epsilon}\; \nabla_\Phi\;\mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon)))
\end{eqnarray*}

\slide{Mutual Information as a Channel Rate}

Typically $z_\Phi(y,\epsilon)$ is simple.  For example

\begin{eqnarray*}
\epsilon & \sim & {\cal N}(0,I) \\
\\
z_\Phi(y,\epsilon) & = & \mu_\Phi(y) +\sigma_\Phi(y)\odot \epsilon
\end{eqnarray*}

\vfill
In this example {\color{red} $p_\Phi(z|y)$ is easily computed.}

\slide{Mutual Information Replaces Rate}

\begin{eqnarray*}
I_\Phi(y,z)  & = & E_{y,\epsilon}\; \ln \frac{\mathrm{pop}(y)p_\Phi(z|y)}{\mathrm{pop}(y)p_{\mathrm{pop},\Phi}(z)} \\
\\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{p_{\mathrm{pop},\Phi}(z)} \\
\\
\\
\mathrm{where}\;\;\;\;p_{\mathrm{pop},\Phi}(z) & = & E_{y\sim \mathrm{pop}}\;\;p_\Phi(z|y)
\end{eqnarray*}

\slide{A Variational Bound}

$$p_{\mathrm{pop},\Phi}(z)  = E_{y\sim \mathrm{pop}}\;\;p_\Phi(z|y)$$

\vfill
We cannot compute $p_{\mathrm{pop},\Phi}(z)$.

\vfill
Instead we will use a model $\hat{p}_\Phi(z)$ to approximate $p_{\popd,\Phi}(z)$.

\slide{A Variational Bound}

\begin{eqnarray*}
{\color{red} I(y,z)}  & = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{p_{\mathrm{pop},\Phi}(z)} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)} + E_{y,\epsilon}\;\ln\frac{\hat{p}_\Phi(z)}{p_{\mathrm{pop},\Phi}(z)} \\
\\
& = & E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)} - KL(p_{\mathrm{pop},\Phi}(z),\hat{p}_\Phi(z)) \\
\\
& {\color{red} \leq} & {\color{red} E_{y,\epsilon}\;\ln \frac{p_\Phi(z|y)}{\hat{p}_\Phi(z)}}
\end{eqnarray*}

\slide{The Noisy Channel RDA}

{\huge
\begin{eqnarray*}
& & \mbox{General Noisy Channel RDA:} \\
\\
\Phi^* & = & \argmin_\Phi E_{y,\epsilon}\;\ln \frac{p_\Phi(z_\Phi(y,\epsilon)|y)}{\hat{p}_\Phi(z_\Phi(y,\epsilon))}
+ \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y,\epsilon))) \\
\\
\\
& & \mbox{Uniform Box Noise (Rounding) RDA:} \\
\\
\Phi^* &  = & \argmin_\Phi \;E_y\;E_{\epsilon \sim [-1/2,1/2]^d} \\
\\
& & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; - \ln \hat{p}_\Phi(z_\Phi(y) + \epsilon) + \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y) + \epsilon)) \\
\\
\end{eqnarray*}
}

\slide{END}

}
\end{document}
