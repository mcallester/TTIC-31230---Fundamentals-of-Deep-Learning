\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge
  \centerline{\bf TTIC 31230,  Fundamentals of Deep Learning}
  \vfill
  \centerline{David McAllester, Autumn   2020}
  \vfill
  \centerline{\bf Mutual Information Co-Training}
  \vfill
  \vfill


\slide{Language is Situated}

When a child hears an utterance the language is often referring to things present in the immediate physical situation.

\vfill
We can model this with a collection of images paired with recordings of descriptions of the image.

\vfill
We can then try to automatically find correspondences between parts of the sound and parts of the image.

\vfill
{\bf Harwath et al., Unsupervised Learning of Spoken Language with Visual Context, NeurIPS 2016.}

\slide{Language is Situated}

\centerline{\includegraphics[height=4.2 in]{\images/cotrain}}

\centerline{Harwath}


\slide{Mutual Information Co-Training}

Here I will formulate mutual information co-training.

\vfill
This gives an abstract objective for multi-modal learning of latent variables.

\vfill
This objective was not followed (explicitly) in Harwath et al.

\slide{Mutual Information Co-Training}

Consider a population distribution on pairs $\tuple{x,y}$.

\vfill
For example $x$ might be an image and $y$ a sound wave.

\vfill
We are interested in extracting latent variables $z_x$ and $z_y$ from $x$ and $y$ respectively.

\vfill
For example $z_x$ might be a bag of words extracted from the image and $z_y$ a bag of words extracted from the sound wave.


\slide{Mutual Information Co-Training}

For a population on $\tuple{x,y}$ we introduce two discrete latent variables $z_x$ and $z_y$ defined by
models {\color{red} $P_\Phi(z_x|x)$} and {\color{red} $P_\Phi(z_y|y)$}.

\vfill
$$\Phi^* = \argmax_{\Phi} \; I_{\pop,\Phi}(z_x,z_y) - \beta (H_{\pop,\Phi}(z_x) + H_{\pop,\Phi}(z_y))$$

\vfill
Here we are asking to maximize the mutual information while (intuitively) limiting the information in $z_x$ and $z_y$.

\vfill
In the bag of words example we are asking to maximize the mutual information between the two probability distributions on bags of words while limiting the information in the bags.

\slide{Mutual Information Co-Training}

$$\Phi^* = \argmax_{\Phi} \; I_{\pop,\Phi}(z_x,z_y) - \beta (H_{\pop,\Phi}(z_x) + H_{\pop,\Phi}(z_y))$$

\vfill
Limiting the information in $z_x$ and $z_y$ prevents the trivial solution of $z_x = x$ and $z_y = y$.

\vfill
{\bf Here we only model distributions on $z_x$ and $z_y$.  Unlike GANs and VAEs, there is no attempt to model distributions on the observables $x$ and $y$.}

\slide{Mutual Information Co-Training}

\begin{eqnarray*}
\Phi^* & = & \argmax_{\Phi} \; I_{\pop,\Phi}(z_x,z_y) - \beta (H_{\pop,\Phi}(z_x) + H_{\pop,\Phi}(z_y)) \\
\\
\\
 & = & \argmax_{\Phi} \left\{\begin{array}{l}\; \frac{1}{2}(H_{\pop,\Phi}(z_x) - H_{\pop,\Phi}(z_x|z_y)) \\
                                              +\;\frac{1}{2}(H_{\pop,\Phi}(z_y) - H_{\pop,\Phi}(z_y|z_x)) \\
                                              - \;\beta (H_{\pop,\Phi}(z_x) + H_{\pop,\Phi}(z_y)) \end{array}\right. \\
\\
\\
 & = & \argmax_{\Phi} \; \left\{\begin{array}{l}(1-2\beta)(H_{\pop,\Phi}(z_x)+ H_{\pop,\Phi}(z_y)) \\ -\; H_{\pop,\Phi}(z_x|z_y) - H_{\pop,\Phi}(z_y|z_x) \end{array}\right.
\end{eqnarray*}

\slide{Mutual Information Co-Training}

\begin{eqnarray*}
\Phi^* & = & \argmax_{\Phi} \; \left\{\begin{array}{l}(1-2\beta)(H_{\pop,\Phi}(z_x)+ H_{\pop,\Phi}(z_y)) \\
\\
-\; H_{\pop,\Phi}(z_x|z_y) - H_{\pop,\Phi}(z_y|z_x) \end{array}\right.
\end{eqnarray*}

\vfill
The above entropies and conditional entropies are defined in terms of the population distribution $\pop$ and the models $P_\phi(z_x|x)$ and $P_\Phi(z_y|y)$.

\vfill
Since the population distribution is unknown, we cannot optimize this directly.

\slide{Mutual Information Co-Training}

{\huge
\begin{eqnarray*}
\Phi^* & = & \argmax_{\Phi} \; \left\{\begin{array}{l}(1-2\beta)(H_{\pop,\Phi}(z_x)+ H_{\pop,\Phi}(z_y)) \\
\\
-\; H_{\pop,\Phi}(z_x|z_y) - H_{\pop,\Phi}(z_y|z_x) \end{array}\right.
\end{eqnarray*}
}

\vfill
We would like to maximize a lower bound on this expression.

\vfill
Entropies are upper bounded by cross entropies.

\vfill
{\huge
\begin{eqnarray*}
\Phi^* & = & \argmax_{\Phi} \; \left\{\begin{array}{l}(1-2\beta)(H_{\pop,\Phi}(z_x)+ H_{\pop,\Phi}(z_y)) \\
\\
-\; \hat{H}_{\Phi}(z_x|z_y) - \hat{H}_{\Phi}(z_y|z_x) \end{array}\right.\\
\\
\hat{H}_\Phi(z_x|z_y) & = & E_{(x,y)\sim \pop,\;z_x\sim P_\Phi(z_x|x),\;z_y \sim P_\Phi(z_y|y)}\;\;-\ln P_\Phi(z_x|z_y)
\end{eqnarray*}
}


\slide{Mutual Information Co-Training}

To do the optimization we can replace all entropies with cross-entropies.

\vfill
{\huge
\begin{eqnarray*}
\Phi^* & = & \argmax_{\Phi} \; \left\{\begin{array}{l}(1-2\beta)(\hat{H}_{\Phi}(z_x)+ \hat{H}_{\Phi}(z_y)) \\
\\
-\; \hat{H}_{\Phi}(z_x|z_y) - \hat{H}_{\Phi}(z_y|z_x) \end{array}\right.
\end{eqnarray*}
}

\vfill
While this is no longer a lower bound on the desired mutual information objective, it might still be useful in practice.

\vfill
This is called a {\bf difference of entropies} objective.

\slide{END}

}
\end{document}

