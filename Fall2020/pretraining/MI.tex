\input ../../SlidePreamble
\input ../../preamble


\begin{document}

{\Huge
  \centerline{\bf TTIC 31230,  Fundamentals of Deep Learning}
  \vfill
  \centerline{David McAllester, Autumn   2020}
  \vfill
  \centerline{\bf Mutual Information Coding}
  \vfill
  \vfill


\slide{Deep Co-Training}

For a population on $\tuple{x,y}$ and a ``feature map'' $z_\Phi$ we optimize $\Phi$ by

\vfill
$$\Phi^* = \argmax_\Phi \; I(z_\Phi(x),z_\Phi(y)) - \beta H(z_\Phi(x))$$


\vfill
Here we can think of $z_\Phi(x)$ as what we remember about a past $x$ to carry information about a future $y$ while maintaining low memory requirements.

\slide{Deep Co-Training}

\begin{eqnarray*}
\Phi^* & = & \argmax_\Phi \; (1-\beta)\hat{H}_\Phi(z_\Phi(x)) - \hat{H}_\Phi(z_\Phi(x)|z_\Phi(y)) \\
\\
\hat{H}_\Phi(z_\Phi(x)) & = & E_x \; -\ln \;P_{\Psi^*(\Phi)}(z_\Phi(x)) \\
\\
\Psi^*(\Phi) & = & \argmin_\Psi\;E_x\;-\ln P_\Psi(z_\Phi(x)) \\
\\
\hat{H}_\Phi(z_\Phi(x)|z_\Phi(y)) & = & E_{x,y} \; -\ln P_\Phi(z_\Phi(x)|z_\Phi(y)) 
\end{eqnarray*}

\vfill
Here we only model distributions on $z$.  Unlike VAEs, there is no attempt to model distributions on $x$ or $y$.

\slide{Mutual Information Objectives}

CPC represents a fundamental shift in the self-supervised training objective.

\vfill
GANs and VAEs are motivated by modeling $\pop(y)$.

\vfill
But in CPC there is no attempt to model $\pop(y)$.

\vfill
CPC can be viewed as training a feature map $z_\Phi$ so as to maximize the mutual information {\color{red} $I(z_\Phi(x),z_\Phi(y))$} while, at the same time, making $z_\Phi(x)$ useful
for linear classifiers.

\slide{Relationship to Noise Contrastive Estimation}

CPC is noise contrastive estimation (NCE) with ``noise'' generated by drawing $y$ unrelated to $x$.
By the NCE theorems, universality implies

$$P_{\Phi^*}(i|z_1,\ldots,z_N,z_x) = \softmax_i \;\ln \frac{\pop(z_i|z_x)}{\pop(z_i)}$$

and also

{\huge
\begin{eqnarray*}
{\cal L}_\mathrm{CPC} & \geq & \ln N - \frac{N-1}{N}(KL(\pop(z_y|z_x),\pop(z_y)) + KL(\pop(z_y),\pop(z_y|z_x))) \\
\\
& = & \ln N - \frac{N-1}{N}({\color{red} I(z_x,z_y)} + KL(\pop(z_y),\pop(z_y|z_x)))
\end{eqnarray*}
}
   
\slide{Contrastive Predictive Coding (CPC)}

We consider a population distribution on pairs $\tuple{x,y}$.

\vfill
For example $x$ and $y$ might be video frames separated by 10 seconds in a video.

\vfill
For simplicity we will assume that the marginal distributions on $x$ and $y$ are the same --- the probability that an image occurs as a first frame
is the same as the probability that image occurs as a second frame.

\vfill
In CPC we draw a pair $\tuple{x,y}$ and {\color{red} minimize} a discriminator loss for distinguishing $z_\Phi(y)$ from $z_\Phi(\tilde{y})$ for $\tilde{y} \sim \pop(y)$.
The discriminator gets to see $x$.

\slide{Contrastive Predictive Coding (CPC)}

For $N \geq 2$ let  $\tilde{P}^{(N)}$ be the distribution on tuples $\tuple{i,y_1,\ldots,y_N,x}$
defined by the following process.

\begin{itemize}
\item draw a pair $\tuple{x,y}$ from the population.

\item drawn a sequence of $N-1$ ``distractor values'' from the marginal distribution $\pop(y)$.  These are unrelated to $x$.

\item insert $y$ at a random position among the distractors to get the sequence
$y_1,\ldots,y_N$.

\item return the tuple $\tuple{i,y_1,\ldots,y_N,x}$ where $i$ is the index of $y$ among the distractors.
\end{itemize}

\slide{Contrastive Predictive Coding (CPC)}

\begin{eqnarray*}
\Phi^*  & = & \argmin_\Phi\;{\cal L}_\mathrm{CPC}(\Phi) \\
\\
{\cal L}_\mathrm{CPC}(\Phi) &= & E_{\tuple{i,y_1,\ldots,y_N,x} \sim \tilde{P}^{(N)}} \\
\\
& & ~\;\;\;\;\;\; -\ln P_\mathrm{CPC}(i|z_\Phi(y_1),\ldots,z_\Phi(y_N),z_\Phi(x))
\end{eqnarray*}

\vfill
$$P_\mathrm{CPC}(i|z_1,\ldots,z_N,z_x) = \softmax_i\;z_i^\top z_x$$

\slide{Contrastive Predictive Coding (CPC)}

\begin{eqnarray*}
\Phi^*  & = & \argmin_\Phi\;{\cal L}_\mathrm{CPC}(\Phi)
\end{eqnarray*}

\vfill
$$P_\Phi(i|z_1,\ldots,z_n,z_x) = \softmax_i\;z_i^\top z_x$$

\vfill
As $N$ gets larger the contrastive discrimination task gets harder.

\vfill
The task is also made difficult by the requirement that the score is defined to be an inner product of feature vectors.

\slide{Contrastive Predictive Coding (CPC)}

(SimCLR:) A Simple Framework for Contrastive Learning of Visual Representations, Chen et al., Feb. 2020 (self-supervised leader as of February, 2020).

\vfill
They use a distribution on pairs $\tuple{x,y}$ defined by drawing an image $s$ from ImageNet and then drawing $x$ and $y$ as random ``augmentations'' (modifications) of the image $s$
--- either a random translation, rotation, color jitter, masking, edge image, or a composition of these modifications.

\slide{Contrastive Predictive Coding (CPC)}

The feature map $z_\Phi$ can then be applied to the images of ImageNet.

\vfill
The feature map $z_\Phi$ is then tested by using a {\color{red} linear} classifier for ImageNet based on these features.

\slide{SimCLR}

\centerline{\includegraphics[height=5.2 in]{\images/SimCLR}}

\slide{END}

}
\end{document}

\slide{END}

}
\end{document}
